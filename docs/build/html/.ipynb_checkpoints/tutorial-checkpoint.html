<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tutorials &mdash; BatCat 0.2.9 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> BatCat
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../philosophy.html">Philosophy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">BatCat API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../appendix.html">Appendix</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute.html">Contribute to BatCat</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">BatCat</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a></li>
      <li class="breadcrumb-item active">Tutorials</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/.ipynb_checkpoints/tutorial-checkpoint.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="tutorials">
<h1>Tutorials<a class="headerlink" href="#tutorials" title="Permalink to this heading"></a></h1>
<div class="section" id="data-science-projects-basics">
<h2>Data Science Projects Basics<a class="headerlink" href="#data-science-projects-basics" title="Permalink to this heading"></a></h2>
<div class="section" id="environment-setup">
<h3>Environment Setup<a class="headerlink" href="#environment-setup" title="Permalink to this heading"></a></h3>
<p>The first step to start a data science project should always be setup a development file system, no matter on cloud or in your laptop. <strong>BatCat</strong> provides a one-line command to setup a well-organized file system for data science projects.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>batcat.FileSys
</pre></div>
</div>
<p>The interactive and immersive command-line interfaces as following. Just type down your project name, like <code class="code docutils literal notranslate"><span class="pre">battery</span></code> in this tutorial. Then it will generate a file structure for your data science project and print out a file tree of it.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>hi, there! please write down your machine learning project&#39;s name.
project&#39;s name: battery
project_battery/
    requirements.txt
    README.md
    .gitignore
    docs/
        READM.md
    log/
    model/
    test/
    data/
        tmp/
        train/
        test/
        result/
        raw/
    notebook/
    report/
    script/
        config.json
    deploy/
        init.py
        config.json
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic simple">
<li><p><code class="file docutils literal notranslate"><span class="pre">requirements.txt</span></code> includes all packages you need in your project. We recommend you to list not only package names but thier versions in the file. Besides, this serves your well if you develop your project on SageMaker, for you have to install all required packages every time restarting the Jupyter Notebook instance.</p></li>
<li><p><code class="file docutils literal notranslate"><span class="pre">.gitignore</span></code> includes <code class="file docutils literal notranslate"><span class="pre">data/*</span></code> by default, which is our best practice in data science projects with <strong>git</strong>. Generally, you don’t want to git your data.</p></li>
<li><p><code class="file docutils literal notranslate"><span class="pre">docs/READM.md</span></code> is inspired by <a class="reference external" href="https://docs.google.com/document/d/16R1E2ExKUCP5SlXWHr-KzbVDx9DBUclra-EbU8IB-iE/edit?usp=sharing">How to ML Paper - A brief Guide</a>. We highly recommend you to document your data science project in an organized way so that anyone, including youself, can catch up your thoughts in the future.</p></li>
</ol>
</div>
</div>
<div class="section" id="logging">
<h3>Logging<a class="headerlink" href="#logging" title="Permalink to this heading"></a></h3>
<p>Most data scientists spend little time on logging and may just print out along the experiement in Jupyter Notebook. However, this can make annoying troubles when it comes to production environment or when the data science experiements require a long period to generate experiement records. Therefore, logging is critical to a data science project.</p>
<p>Python Module <strong>Logging</strong> is one of the most underrated features. Two things (5&amp;3) to take away from <strong>Logging</strong>:</p>
<ol class="arabic simple">
<li><p><strong>5 levels</strong> of importance that logs can contain(debug, info, warning, error, critical);</p></li>
<li><p><strong>3 components</strong> to configure a logger in Python (a logger, a formatter, and at least one handler).</p></li>
</ol>
<p><strong>BatCat</strong> provides a function <code class="code docutils literal notranslate"><span class="pre">get_logger</span></code> to make life easier.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">batcat</span> <span class="k">as</span> <span class="nn">bc</span>

<span class="n">log_name</span> <span class="o">=</span> <span class="s1">&#39;battery&#39;</span>
<span class="n">log_file</span> <span class="o">=</span> <span class="s1">&#39;../log/batter.log&#39;</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">bc</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="n">logName</span><span class="o">=</span><span class="n">log_name</span><span class="p">,</span> <span class="n">logFile</span><span class="o">=</span><span class="n">log_file</span><span class="p">)</span>

<span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="s1">&#39;this is a debug&#39;</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;this is a test&#39;</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s1">&#39;this is a warning&#39;</span><span class="p">)</span>

<span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s1">&#39;this is an error!&#39;</span><span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">critial</span><span class="p">(</span><span class="s1">&#39;this is critical!&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="clean-code">
<h3>Clean Code<a class="headerlink" href="#clean-code" title="Permalink to this heading"></a></h3>
<p>We recommend <a class="reference external" href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pipe.html">pipe</a> of <code class="code docutils literal notranslate"><span class="pre">pandas</span></code> to structure your machine learning or data science code. The following is a sample code for inference, which we structure the <strong>preprocessing</strong> and <strong>feature engineering</strong> tasks as pipe functions and stack them together as a pipeline.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">batcat</span> <span class="k">as</span> <span class="nn">bc</span>

<span class="c1"># read data</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">bc</span><span class="o">.</span><span class="n">read_csv_from_bucket</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

<span class="c1"># preprocessing and feature engineering</span>
<span class="n">df</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">pipe</span><span class="p">(</span><span class="n">func1</span><span class="p">)</span>
        <span class="o">.</span><span class="n">pipe</span><span class="p">(</span><span class="n">func2</span><span class="p">,</span> <span class="n">arg1</span><span class="o">=</span><span class="n">a</span><span class="p">)</span>
        <span class="o">.</span><span class="n">pipe</span><span class="p">(</span><span class="n">func3</span><span class="p">,</span> <span class="n">arg2</span><span class="o">=</span><span class="n">b</span><span class="p">,</span> <span class="n">arg3</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># inference</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

<span class="c1"># save results</span>
<span class="n">bc</span><span class="o">.</span><span class="n">save_to_bucket</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">bucket</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The code above utilize the I/O tolls of <strong>BatCat</strong>, which provided as following section.</p>
</div>
</div>
</div>
<div class="section" id="io-tools">
<h2>IO Tools<a class="headerlink" href="#io-tools" title="Permalink to this heading"></a></h2>
<p><strong>Services on AWS</strong>: S3, Redshift, Athena.</p>
<p><strong>BatCat</strong> supports reading data from S3 bucket (directly or by Athena or Redshift) and saving back to S3.</p>
<div class="section" id="s3-bucket">
<h3>S3 Bucket<a class="headerlink" href="#s3-bucket" title="Permalink to this heading"></a></h3>
<p>Read CSV data directly from S3 and save a DataFrame to S3.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">bucket</span> <span class="o">=</span> <span class="s1">&#39;2022-RnD-battery&#39;</span>
<span class="n">key</span> <span class="o">=</span> <span class="s1">&#39;usage&#39;</span>

<span class="c1"># from s3</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">bc</span><span class="o">.</span><span class="n">read_csv_from_bucket</span><span class="p">(</span><span class="n">bucket</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>

<span class="c1"># to s3</span>
<span class="n">bc</span><span class="o">.</span><span class="n">save_to_bucket</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">bucket</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="sql-redshift-athena">
<h3>SQL: Redshift, Athena<a class="headerlink" href="#sql-redshift-athena" title="Permalink to this heading"></a></h3>
<p>The above approach is fine with a given S3 object but can be tricky when it comes to scenarios you need write SQLs to query data. This can be handled with Athena and Redshift.</p>
<ol class="arabic simple">
<li><dl class="simple">
<dt><strong>Redshift</strong>:</dt><dd><ul class="simple">
<li><p>Option 1: With host/password.</p></li>
<li><p>Option 2: With Secrets Manager.</p></li>
</ul>
</dd>
</dl>
</li>
<li><p><strong>Athena</strong>: Service Glue is required before you query with Athena.</p></li>
</ol>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># from RedShift</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">SELECT</span>
<span class="s2">    vin,</span>
<span class="s2">    usage,</span>
<span class="s2">    time</span>
<span class="s2">FROM</span>
<span class="s2">    cdc_dw_bms.battery_usage</span>
<span class="s2">WHERE</span>
<span class="s2">    time &gt;= &#39;</span><span class="si">{}</span><span class="s2">&#39; and time &lt;= &#39;</span><span class="si">{}</span><span class="s2">&#39;</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">date_start</span> <span class="o">=</span> <span class="s1">&#39;2022-01-01&#39;</span>
<span class="n">date_end</span> <span class="o">=</span> <span class="s1">&#39;2022-08-01&#39;</span>

<span class="c1">## with host/password</span>
<span class="n">host</span> <span class="o">=</span> <span class="s1">&#39;0.1.1.1&#39;</span>
<span class="n">password</span> <span class="o">=</span> <span class="s1">&#39;this_is_a_password&#39;</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">bc</span><span class="o">.</span><span class="n">read_data_from_redshift</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
                                <span class="n">host</span><span class="o">=</span><span class="n">host</span><span class="p">,</span>
                                <span class="n">password</span><span class="o">=</span><span class="n">password</span><span class="p">,</span>
                                <span class="n">port</span><span class="o">=</span><span class="mi">5439</span><span class="p">,</span>
                                <span class="n">database</span><span class="o">=</span><span class="s1">&#39;dev&#39;</span><span class="p">,</span>
                                <span class="n">user</span><span class="o">=</span><span class="s1">&#39;awsuser&#39;</span><span class="p">,</span>
                                <span class="n">date_start</span><span class="o">=</span><span class="n">date_start</span><span class="p">,</span>
                                <span class="n">date_end</span><span class="o">=</span><span class="n">date_end</span><span class="p">)</span>

<span class="c1">## with secrets manager</span>
<span class="n">secret_name</span> <span class="o">=</span> <span class="s1">&#39;secret/manager&#39;</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">bc</span><span class="o">.</span><span class="n">read_data_from_redshift_by_secret</span><span class="p">(</span><span class="n">secret_name</span><span class="o">=</span><span class="n">secret_name</span><span class="p">,</span>
                                          <span class="n">region</span><span class="o">=</span><span class="n">region</span><span class="p">,</span>
                                          <span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">)</span>


<span class="c1"># to RedShift</span>
<span class="n">schema</span> <span class="o">=</span> <span class="s1">&#39;your_result_schema_name&#39;</span>
<span class="n">table_name</span> <span class="o">=</span> <span class="s1">&#39;your_result_table_name&#39;</span>

<span class="n">bc</span><span class="o">.</span><span class="n">save_df_to_redshift</span><span class="p">(</span><span class="n">df</span><span class="p">,</span>
                       <span class="n">host</span><span class="o">=</span><span class="n">host</span><span class="p">,</span> <span class="n">password</span><span class="o">=</span><span class="n">password</span><span class="p">,</span> <span class="n">port</span><span class="o">=</span><span class="mi">5439</span><span class="p">,</span> <span class="n">database</span><span class="o">=</span><span class="s1">&#39;dev&#39;</span><span class="p">,</span> <span class="n">user</span><span class="o">=</span><span class="s1">&#39;awsuser&#39;</span><span class="p">,</span>
                       <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">schema</span><span class="p">)</span>
</pre></div>
</div>
<p>The functions above are based on the package <strong>redshift_connector</strong> but more user-friendly for data scientists. You can read data from and save it to RedShift in your data science projects.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic simple">
<li><p>If you don’t specify <code class="code docutils literal notranslate"><span class="pre">schema</span></code> with <code class="code docutils literal notranslate"><span class="pre">bc.save_df_to_redshift</span></code>, it will save to <code class="code docutils literal notranslate"><span class="pre">public</span></code> by default, which is not recommended for database management reason.</p></li>
<li><p>You may want to add a <strong>timestamp column</strong> to your dataframe with <code class="code docutils literal notranslate"><span class="pre">bc.save_df_to_redshift</span></code> so that you can distinguish potential duplicated rows with <code class="code docutils literal notranslate"><span class="pre">if_exist=&quot;append&quot;</span></code>.</p></li>
</ol>
</div>
<p>Unlike RedShift, Athena is a serverless service and does not need any infrastructure to create, manage, or scale data sets. It works directly on top of Amazon S3 data sets. It creates external tables and therefore does not manipulate S3 data sources, working as a read-only service from an S3 perspective.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># via Athena</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">SELECT</span>
<span class="s2">    vin,</span>
<span class="s2">    usage,</span>
<span class="s2">    time</span>
<span class="s2">FROM</span>
<span class="s2">    cdc.dw_bms.battery_usage</span>
<span class="s2">WHERE</span>
<span class="s2">    time &gt;= &#39;</span><span class="si">{}</span><span class="s2">&#39; and time &lt;= &#39;</span><span class="si">{}</span><span class="s2">&#39;</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">date_start</span> <span class="o">=</span> <span class="s1">&#39;2022-01-01&#39;</span>
<span class="n">date_end</span> <span class="o">=</span> <span class="s1">&#39;2022-08-01&#39;</span>

<span class="n">region</span> <span class="o">=</span> <span class="s1">&#39;cn-northwest-1&#39;</span>
<span class="n">s3_staging_dir</span> <span class="o">=</span> <span class="s2">&quot;s3://apac-athena-queryresult/ATHENA_QUERY&quot;</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">bc</span><span class="o">.</span><span class="n">read_data_from_athena</span><span class="p">(</span><span class="n">query</span><span class="o">=</span><span class="n">query</span><span class="p">,</span>
                              <span class="n">region</span><span class="o">=</span><span class="n">region</span><span class="p">,</span>
                              <span class="n">s3_staging_dir</span><span class="o">=</span><span class="n">s3_staging_dir</span><span class="p">,</span>
                              <span class="n">date_start</span><span class="o">=</span><span class="n">date_start</span><span class="p">,</span>
                              <span class="n">date_end</span><span class="o">=</span><span class="n">date_end</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic simple">
<li><dl class="simple">
<dt>Pay attention to the queries for RedShift and Athena are different.</dt><dd><ul class="simple">
<li><p><strong>RedShift</strong>: <code class="code docutils literal notranslate"><span class="pre">[datasource]_[database]</span></code> as schema.</p></li>
<li><p><strong>Athena</strong>: <code class="code docutils literal notranslate"><span class="pre">[datasource].[database]</span></code></p></li>
</ul>
</dd>
</dl>
</li>
<li><p>As Athena works directly on top of Amazon S3 data sets, you may save your results to S3 with tools <a class="reference internal" href="../tutorial.html#io-tools"><span class="std std-ref">directly saving to S3 Bucket</span></a> .</p></li>
</ol>
</div>
</div>
</div>
<div class="section" id="deployment-on-cloud">
<h2>Deployment on Cloud<a class="headerlink" href="#deployment-on-cloud" title="Permalink to this heading"></a></h2>
<p>We notice there are many combinations of services on AWS can serve the machine learning deployment, like 1) Lambda, ECR, 2) Lambda, EFS, 3) Lambda, SageMaker, Step Functions, etc. Here we provide one practice as following.</p>
<p><strong>Services on AWS</strong>: ECR, SageMaker Processing, Step Functions, and Lambda.</p>
<div class="section" id="background">
<h3>Background<a class="headerlink" href="#background" title="Permalink to this heading"></a></h3>
<p>Before we dive in the topic, let’s align on the meaning of “deployment on cloud”. This basicly involves <strong>microservice</strong> like container and <strong>serverless</strong>. In the AWS context, it related services:</p>
<ul class="simple">
<li><p>ECR</p></li>
<li><p>SageMaker Processing</p></li>
<li><p>Step Functions</p></li>
<li><p>Lambda</p></li>
<li><p>IAM</p></li>
</ul>
<p>Amazon SageMaker lets developers and data scientists train and deploy machine learning models. With Amazon SageMaker Processing, you can run processing jobs for data processing steps in your machine learning pipeline.</p>
<p>However, the most annoying part of SageMaker is that it offers many modules <em>to faciliate</em> model development and deployment but looks like a white elephant. What a data scientist need is something with shallow learning curve and the knowledge can be transfered to other cloud services, <strong>NOT</strong> something only works on AWS, which betrays the intend to use Docker!</p>
<p>So here’s BatCat. It provides templates to setup docker images, workflows of Step Functions, and triggers generated by Lambda functions – to slim down the setup work on AWS.</p>
<img alt=".ipynb_checkpoints/images/process.svg" class="align-center" src=".ipynb_checkpoints/images/process.svg" /><p><strong>BatCat</strong> takes all steps in a machine learning product as processing jobs – data cleaning, preprocessing, feature engineering, predicting. Note that the training step is not in production stage but development stage so not inlcuded here.</p>
</div>
<div class="section" id="initialize">
<h3>Initialize<a class="headerlink" href="#initialize" title="Permalink to this heading"></a></h3>
<ol class="arabic simple">
<li><dl class="simple">
<dt>Create related roles and attach policies to it.</dt><dd><p>Like any other AWS services, roles and policies setup is one of the most disappointing parts when using it. Refer to <a class="reference internal" href="../appendix.html#identity-and-access-management-iam"><span class="std std-ref">Identity and Access Management</span></a> for more information.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Go to <code class="file docutils literal notranslate"><span class="pre">deploy/</span></code> folder and setup templates:</dt><dd><ol class="arabic simple">
<li><p>Revise <code class="file docutils literal notranslate"><span class="pre">config.json</span></code> <strong>BatCat</strong> generated for your purpose (check the code below).</p></li>
<li><p>Run <code class="file docutils literal notranslate"><span class="pre">init.py</span></code> script.</p></li>
</ol>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Add your data science core script.</dt><dd><ol class="arabic simple">
<li><p>Add your data science Python script to the current directory, whose name should aligned with <code class="code docutils literal notranslate"><span class="pre">purpose</span></code>. In the example below, it is <code class="code docutils literal notranslate"><span class="pre">usage-analysis.py</span></code>.</p></li>
<li><p>Revise your output destination with <code class="code docutils literal notranslate"><span class="pre">bc.processing_output_path</span></code>.</p></li>
</ol>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Run the scripts to deploy.</dt><dd><p>That’s it!</p>
</dd>
</dl>
</li>
</ol>
<p>All configurations you need to setup are stored in <code class="file docutils literal notranslate"><span class="pre">config.json</span></code>.</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">&quot;project&quot;</span><span class="p">:</span><span class="w">  </span><span class="s2">&quot;2022-RnD-battery&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;purpose&quot;</span><span class="p">:</span><span class="w">  </span><span class="s2">&quot;inference&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;result_s3_bucket&quot;</span><span class="p">:</span><span class="w">  </span><span class="s2">&quot;2022-RnD-battery&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;partition&quot;</span><span class="p">:</span><span class="w">  </span><span class="s2">&quot;aws-cn&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="nt">&quot;workflow_execution_role&quot;</span><span class="p">:</span><span class="w">  </span><span class="s2">&quot;arn:[partition]:iam::[account-id]:role/[role-name]&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ol class="arabic simple">
<li><p><code class="code docutils literal notranslate"><span class="pre">project</span></code>: your data science project name. We suggest a format as <code class="code docutils literal notranslate"><span class="pre">[year]-[domain]-[topic]</span></code>.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">purpose</span></code>: or subproject under a project.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">result_s3_bucket</span></code>: the S3 bucket to store data science results.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">workflow_execution_role</span></code>: the role ARN you created in step 1.</p></li>
</ol>
</div>
<p>Setup a result path within container so that the Step Functions can find and save the output to S3 later.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">batcat</span> <span class="k">as</span> <span class="nn">bc</span>

<span class="n">output_path</span> <span class="o">=</span> <span class="n">bc</span><span class="o">.</span><span class="n">processing_output_path</span><span class="p">(</span><span class="n">purpose</span><span class="p">,</span> <span class="n">timestamp</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">local</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">results</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">output_path</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Then we obtain a <code class="file docutils literal notranslate"><span class="pre">deploy/</span></code> file structure as following.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">deploy</span><span class="o">/</span>
    <span class="mi">2022</span><span class="o">-</span><span class="n">RnD</span><span class="o">-</span><span class="n">battery</span><span class="o">-</span><span class="n">inference</span><span class="o">-</span><span class="n">trigger</span><span class="o">/</span>
        <span class="n">lambda_function</span><span class="o">.</span><span class="n">py</span>
    <span class="n">docker</span><span class="o">/</span>
        <span class="n">Dockerfile</span>
        <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">inference</span><span class="o">.</span><span class="n">py</span>
    <span class="n">init</span><span class="o">.</span><span class="n">py</span>
    <span class="n">config</span><span class="o">.</span><span class="n">json</span>
    <span class="n">requirements</span><span class="o">.</span><span class="n">txt</span>
    <span class="n">setup_docker</span><span class="o">.</span><span class="n">sh</span>
    <span class="n">setup_stepfunctions_inference</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
</div>
<div class="section" id="deploy-it">
<h3>Deploy it!<a class="headerlink" href="#deploy-it" title="Permalink to this heading"></a></h3>
<ol class="arabic simple">
<li><p>Add your Python packages to the <code class="file docutils literal notranslate"><span class="pre">requirements.txt</span></code>.</p></li>
<li><p>Run <code class="file docutils literal notranslate"><span class="pre">setup_docker.sh</span></code>.</p></li>
<li><p>Run <code class="file docutils literal notranslate"><span class="pre">setup_stepfunctions_inference.py</span></code>.</p></li>
<li><p>Go to <strong>Step Functions</strong> on AWS to check the status.</p></li>
<li><p>Go to <strong>Lambda</strong> on AWS and copy the <code class="file docutils literal notranslate"><span class="pre">lambda_function.py</span></code> to it.</p></li>
<li><p>Go to <strong>Cloud Watch</strong> on AWS to set up an <strong>Event Bus</strong> for the trigger.</p></li>
</ol>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Ewen Wang.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>